# High Availability PostgreSQL on Kubernetes using Cloud Native

If you have ever tried to run a production-grade database in a home Kubernetes cluster, you know the pain. You start with a simple Pod, then you need persistence. Then you realize you need backups. Then you want High Availability (HA) because you're tired of GitLab going down every time you patch your server.

This project is the result of battling through the common pitfalls of running PostgreSQL on Kubernetes. We moved away from the "easy" Bitnami charts (due to recent licensing changes) and raw StatefulSets (which are a nightmare to manage) to a robust, operator-based solution using CloudNativePG (CNPG).

The "Why": The Issues We Faced

Before we get to the code, let's look at the specific dragons we had to slay to get this working.

## 1. The "Bitnami Situation"

For years, bitnami/postgresql-ha was the default choice. Recently, their licensing changes and movement of images to "legacy" repos broke many home lab pipelines. We needed a truly open-source, CNCF-graduated alternative that didn't rely on a vendor's shifting catalog.

## 2. The StatefulSet Trap

I initially wanted to write a raw Kubernetes StatefulSet. While great for learning, this is a trap for databases.

Failover is hard: If the Primary node dies, who tells the Replicas to promote themselves? A StatefulSet script can't easily do this safely without causing a "split-brain" scenario.

Backups are manual: You end up writing fragile cronjobs to pg_dump data.

## The Solution: CloudNativePG + MinIO

We settled on CloudNativePG (CNPG). It is an Operator pattern, meaning a piece of software runs inside your cluster specifically to manage Postgres.

Automated Failover: It monitors the primary. If it dies, it promotes a replica instantly.

Continuous Archiving: Instead of nightly dumps, it streams the "Write Ahead Log" (WAL) to MinIO (S3) in real-time. This means Point-in-Time Recovery (you can roll back the DB to exactly 3:42 PM yesterday).

Self-Healing: If a PVC corrupts, it rebuilds the node from the S3 backup automatically.

# Deployment Guide

Prerequisites

Kubernetes Cluster (k3s, Talos, etc.)

CloudNativePG Operator Installed:
``` bash
helm repo add cnpg [https://cloudnative-pg.io/charts](https://cloudnative-pg.io/charts)
helm upgrade --install cnpg --namespace cnpg-system --create-namespace cnpg/cloudnative-pg
```

## MinIO (for backups):
### Step 1: Configure Backups (MinIO)

We use an S3-compatible backend for WAL archiving. Ensure your MinIO buckets are created.

If you are using the MinIO chart we built alongside this:

#### This creates the buckets automatically on install
```bash
helm install minio minio/minio --namespace storage -f minio-values.yaml
```
Required Buckets:

my-postgres-backups (For the database WAL files)

### Step 2: Create the S3 Credentials Secret

Postgres needs to authenticate with MinIO to push backups.
```bash
kubectl create secret generic home-db-s3-creds \
--namespace postgresql \
--from-literal=ACCESS_KEY_ID=minio-admin \
--from-literal=SECRET_ACCESS_KEY=CHANGE_ME_SECRET_KEY
```

### Step 3: Deploy the Postgres Cluster

We use a Helm chart wrapper to deploy the Cluster resource.

Key Configuration Overrides (values.yaml):
```yaml
instances: 3  # 1 Primary, 2 Replicas
storage:
  size: "10Gi"
  enableSuperuserAccess: true # Critical for the init-job to work later
backup:
  enabled: true
  s3:
  bucket: "my-postgres-backups"
  endpoint: "minio.minio.svc.cluster.local:9000"
  insecure: true
```

Install:
```bash
helm install home-db ./postgres-chart -n postgresql --create-namespace
```


### Step 4: Provisioning Databases (The "GitOps" Way)

Do not manually exec into pods to create databases. We created a Kubernetes Job to handle this safely using the auto-generated Superuser secret.

Wait for the Cluster:
```bash
kubectl get cluster -n postgresql
```
## Wait until Status is "Cluster in healthy state"


Run the Creation Job:
Apply the create-gitlab-db.yaml manifest we generated.
```bash
kubectl apply -f create-gitlab-db.yaml
```

Connects to cluster-database-rw (Read-Write service).

Uses the cluster-superuser secret (auto-generated by CNPG).

Creates user new_user and database for a given application.
